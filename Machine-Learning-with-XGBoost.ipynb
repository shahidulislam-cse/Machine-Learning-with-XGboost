{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_with_XGBoost.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Ijg5wUCTQYG"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
        "</p>\n",
        "<br><br>\n",
        "\n",
        "# **Machine Learning with XGboost**\n",
        "\n",
        "Welcome to this hands-on training, where we will learn how to use XGBoost to create powerful prediction models using gradient boosting. Using Jupyter Notebooks you'll learn how to create, evaluate, and tune XGBoost models efficiently. This session will run for three hours, allowing you time to really immerse yourself in the subject, and includes short breaks and opportunities to ask the expert questions throughout the training. \n",
        "\n",
        "You will learn how to:\n",
        "\n",
        "- How to instantiate and customize XGBoost models\n",
        "- How to use XGBoost's DMatrix to optimize performance\n",
        "- How to evaluate models in XGBoost using the right metrics\n",
        "- How to tune parameters in XGBoost to achieve the best results\n",
        "- How to visualize trees in XGBoost to analyze feature importance\n",
        "\n",
        "\n",
        "## **The Dataset**\n",
        "\n",
        "The session's dataset is a CSV file named `hotel_bookings_clean.csv`, which contains data on hotel bookings. \n",
        "\n",
        "### **Acknowledgements**\n",
        "The dataset was downloaded on [Kaggle](https://www.kaggle.com/jessemostipak/hotel-booking-demand/). The data is originally from an article called [Hotel booking demand datasets](https://www.sciencedirect.com/science/article/pii/S2352340918315191) by Nuno Antonio, Ana de Almeida, and Luis Nunes. It was then cleaned by Thomas Mock and Antoine Bichat for [#TidyTuesday during the week of February 11th, 2020](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md). For the purposes of this live training, it was further pre-processed to have cleaner ready-to-use features (e.g., dropping irrelevant columns, one-hot-encoding). The dataset has the following [license](https://creativecommons.org/licenses/by/4.0/).\n",
        "\n",
        "### **Data Dictionary**\n",
        "\n",
        "It contains the **53 columns**:\n",
        "\n",
        "_For binary variables: `1` = true and `0` = false._\n",
        "\n",
        "#### **Target** \n",
        "- `is_cancelled`: Binary variable indicating whether a booking was canceled\n",
        "\n",
        "#### **Features** \n",
        "- `lead time`: Number of days between booking date and arrival date\n",
        "- `arrival_date_week_number`, `arrival_date_day_of_month`, `arrival_date_month`: Week number, day date, and month number of arrival date \n",
        "- `stays_in_weekend_nights`, `stays_in_week_nights`: Number of weekend nights (Saturday and Sunday) and weeknights (Monday to Friday) the customer booked\n",
        "- `adults`,`children`,`babies`: Number of adults, children, babies booked for the stay\n",
        "- `is_repeated_guest`: Binary variable indicating whether the customer was a repeat guest \n",
        "- `previous_cancellations`: Number of prior bookings that were canceled by the customer\n",
        "- `previous_bookings_not_canceled`: Number of prior bookings that were not canceled by the customer\n",
        "- `required_car_parking_spaces`: Number of parking spaces requested by the customer\n",
        "- `total_of_special_requests`: Number of special requests made by the customer\n",
        "- `avg_daily_rate`: Average daily rate, as defined by dividing the sum of all lodging transactions by the total number of staying nights\n",
        "- `booked_by_company`: Binary variable indicating whether a company booked the booking\n",
        "- `booked_by_agent`: Binary variable indicating whether an agent booked the booking\n",
        "- `hotel_City`: Binary variable indicating whether the booked hotel is a \"City Hotel\"\n",
        "- `hotel_Resort`: Binary variable indicating whether the booked hotel is a \"Resort Hotel\"\n",
        "- `meal_BB`: Binary variable indicating whether a bed & breakfast meal was booked \n",
        "- `meal_HB`: Binary variable indicating whether a half board meal was booked\n",
        "- `meal_FB`: Binary variable indicating whether a full board meal was booked \n",
        "- `meal_No_meal`: Binary variable indicating whether there was no meal package booked \n",
        "- `market_segment_Aviation`, `market_segment_Complementary`, `market_segment_Corporate`, `market_segment_Direct`, `market_segment_Groups`, `market_segment_Offline_TA_TO`, `market_segment_Online_TA`, `market_segment_Undefined`: Indicates market segment designation with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators\n",
        "- `distribution_channel_Corporate`, `distribution_channel_Direct`, `distribution_channel_GDS`, `distribution_channel_TA_TO`, `distribution_channel_Undefined`: Indicates booking distribution channel with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators, \"GDS\" = Global Distribution System\n",
        "- `reserved_room_type_A`,`reserved_room_type_B`, `reserved_room_type_C`,`reserved_room_type_D`, `reserved_room_type_E`, `reserved_room_type_F`, `reserved_room_type_G`, `reserved_room_type_H`, `reserved_room_type_L`: Indicates code of room type reserved with a value of `1`. Code is presented instead of designation for anonymity reasons\n",
        "- `deposit_type_No_Deposit`: Binary variable indicating whether a deposit was made\n",
        "- `deposit_type_Non_Refund`: Binary variable indicating whether a deposit was made in the value of the total stay cost\n",
        "- `deposit_type_Refundable`: Binary variable indicating whether a deposit was made with a value under the total stay cost \n",
        "- `customer_type_Contract`: Binary variable indicating whether the booking has an allotment or other type of contract associated to it \n",
        "- `customer_type_Group`: Binary variable indicating whether the booking is associated to a group \n",
        "- `customer_type_Transient`: Binary variable indicating whether the booking is not part of a group or contract, and is not associated to other transient booking\n",
        "- `customer_type_Transient-Party`: Binary variable indicating whether the booking is transient, but is associated to at least another transient booking\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMYfcKeDY85K"
      },
      "source": [
        "## **1. Getting to know our data**\n",
        "\n",
        "Let's get to know our columns and split our data into features and labels!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMQfyC7GUNhT",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import xgboost as xgb # XGBoost typically uses the alias \"xgb\"\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8t_EwRNZPLB",
        "colab": {}
      },
      "source": [
        "# Read in the dataset\n",
        "bookings = pd.read_csv('https://raw.githubusercontent.com/datacamp/Machine-Learning-With-XGboost-live-training/master/data/hotel_bookings_clean.csv')\n",
        "\n",
        "# List out our columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IAfz_jiu0NjN",
        "colab": {}
      },
      "source": [
        "# Take a closer look at column distributions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z_ckvohjegrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot cancellation counts to visualize proportion of not cancelled and cancelled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzk8fFnFegr2",
        "colab_type": "text"
      },
      "source": [
        "Remember for our binary variables, like `is_canceled`, `1` = true and `0` = false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd6-3_wcegr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get an exact percentage of not cancelled and cancelled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEiLAW0regsB",
        "colab_type": "text"
      },
      "source": [
        "### **Which features are most correlated to cancelations?**\n",
        "\n",
        "#### **Correlation Coefficient**\n",
        "- Quantifies the linear relationship between two variables\n",
        "- Number between -1 and 1\n",
        "- Magnitude corresponds to strength of relationship\n",
        "- Sign (+ or -) corresponds to direction of relationship\n",
        "- Most common way to calculate: **Pearson product-moment correlation coefficient**\n",
        "\n",
        "<br>\n",
        "\n",
        "$${\\large\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "![Plots displaying different levels of correlation](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/correlation.png?raw=true)\n",
        "\n",
        "We can use `pandas`'s [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) `DataFrame.corr()` which returns a correlation matrix using the Pearson correlation coefficient as default. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el1FDRvVegsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute correlation matrix \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkagjJQVegsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Which features are most correlated with is_cancelled?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYHZwXTXegsI",
        "colab_type": "text"
      },
      "source": [
        "Be cautious; correlation does not equal feature importance! Correlation may not necessarily help differentiate classes. Also, the Pearson coefficient only considers linear relationships, and some of these variables are binary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skna413fegsI",
        "colab_type": "text"
      },
      "source": [
        "### **Splitting data**\n",
        "Let's split our label and features so we can get to building models! The first column is our target label `is_cancelled`. The rest are features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNORyt4JegsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define X and y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL1ebqkoy-aE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 1</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9m1kfpxegsL",
        "colab_type": "text"
      },
      "source": [
        "## **2. Your First XGBoost Classifier**\n",
        "\n",
        "XGBoost has a  [scikit-learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn), which is useful if you want to use different scikit-learn classes and methods on an XGBoost model (e.g.,`predict()`, `fit()`).  In this section, we'll try the API out with the `xgboost.XGBClassifier()` class and get a baseline accuracy for the rest of our work. So that our results are reproducible, we'll set the `random_state=123`.\n",
        "\n",
        "As a reminder, gradient boosting sequentially trains weak learners where each weak learner tries to correct its predecessor's mistakes. First, we'll instantiate a simple XGBoost classifier without changing any of the other parameters, and we'll inspect the parameters that we haven't touched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zMyC5qkegsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Train and test split using sklearn\n",
        "\n",
        "\n",
        "# Instatiate a XGBClassifier \n",
        "\n",
        "\n",
        "# Inspect the parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3akjIVVegsP",
        "colab_type": "text"
      },
      "source": [
        "There's a couple of things to note:\n",
        "- The `booster` parameter is `gbtree`.  This means the weak learners, or boosters, are decision trees in this model. `gbtree` is the default, and we will keep it this way.\n",
        "- The `objective` function, or loss function, is defined as `binary:logistic`. The objective function quantifies how far off a prediction is from the actual results. We want to minimize this to have the smallest possible loss. `binary:logistic` is the default for classifiers. `binary:logistic` outputs the actual predicted probability of the positive class (in our case, that a booking is cancelled).\n",
        "- `n_estimators` is the number of gradient boosted trees we want in our model. It's equivalent to the number of boosting rounds. For our purposes, we don't want too many boosting rounds, or training will take too long. **Let's lower it from 100 to 10**.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/n_estimators.png?raw=true\" width = \"80%\"> \n",
        "</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "- `max_depth` is the maximum tree depth allowed. Tree depth is the length of the longest path from the root node to a leaf node. Making this too high will give our model more variance, or more potential to overfit. Similar to `n_estimators`, the more we increase this, the longer our training period will be. **Let's keep this at 3**.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/max_depth.png?raw=true\" width = \"50%\"> \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcHSOw6FegsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set n_estimators to be 10 instead of 100\n",
        "\n",
        "\n",
        "# Fit it to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RkrUrV5egsS",
        "colab_type": "text"
      },
      "source": [
        "We are going to use **accuracy** as our metric for this problem since we only have two classes, and the smallest class takes ~37% of the data. To calculate accuracy, we need to count the correctly predicted data points and divide it by the total number of data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqq2x7fegsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the accuracy\n",
        "\n",
        "\n",
        "# Print the baseline accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvfTM4FBegsW",
        "colab_type": "text"
      },
      "source": [
        "### **Visualizing your tree**\n",
        "\n",
        "`XGBoost` has two handy visualization functions for interpreting results.\n",
        "\n",
        "The first is `plot_importance()`  which plots feature importance, meaning, how predictive each feature is for the target variable. It takes in the fitted XGBoost model fitted.\n",
        "\n",
        "\n",
        "#### **Plotting feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPtO3AHegsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBbBCvs5egsa",
        "colab_type": "text"
      },
      "source": [
        "How is importance calculated? Here's an excerpt from the [documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting):\n",
        "\n",
        "> **importance_type (str, default \"weight\") –**\n",
        "> How the importance is calculated: either “weight”, “gain”, or “cover”\n",
        "> - ”weight” is the number of times a feature appears in a tree\n",
        "> - ”gain” is the average gain of splits which use the feature\n",
        "> - ”cover” is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n",
        "\n",
        "We'll focus on the first two. Gain is way to quantify how much a feature contributes to improving accuracy. Let's try out `gain` to see how it differs from `weight`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erGG0M4xegsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot gain instead of weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRzPiziPegsd",
        "colab_type": "text"
      },
      "source": [
        "Here we see the ordering of features differs quite a bit between `gain` and `weight`! This implies that (1) feature importance can be subjective, and (2) the number of appearances a feature has on a tree is not necessarily correlated to how much gain it brings. For example, a binary variable has less of a chance to appear as many times as a continuous variable on a tree, since there are only two outputs. However, it can still be a powerful feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS4JqtMgegsd",
        "colab_type": "text"
      },
      "source": [
        "#### **Plotting a decision tree from your model**\n",
        "\n",
        "The second handy visualization function is `plot_tree()` which visually generates a decision tree in your model. Remember, there are multiple trees in the model.\n",
        "\n",
        "It takes in the fitted XGBoost model, and with the `num_trees` parameter, you can indicate which tree you want to see. For example, if I have `n_estimator=3`, I can set the `num_trees` parameter as `0`, `1`, or `2` to see the first, second, or third tree, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oJtm9NNegse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matplotlib.rcParams['figure.figsize'] = (20.0, 8)\n",
        "\n",
        "# Plot the first tree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2S7DZ-Cegsh",
        "colab_type": "text"
      },
      "source": [
        "The leaf output is the raw score. It's converted to a probability at the end of training.\n",
        "\n",
        "There's a parameter called `rankdir`. We can set it to `LR` so the decision tree builds left to right, as opposed to top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BCG4Bgvegsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the last tree\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngKxd01BzDv6",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 2</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAco9hlqegsm",
        "colab_type": "text"
      },
      "source": [
        "## **3. Cross Validation in XGBoost**\n",
        "\n",
        "Cross validation is considered best practice for assessing a model's performance. We can use `xgboost.cv()` to efficiently run cross validation on XGBoost models. This method is part of XGBoost's core library and **not** part of XGBoost's scikit-learn API from earlier. \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/cv.png?raw=true\" width = \"80%\"> \n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9U4DATFegsm",
        "colab_type": "text"
      },
      "source": [
        "### **Converting to DMartix**\n",
        "\n",
        "**DMatrix is XGBoost's internal data structure designed to optimize for both memory efficiency and training speed.** It's one of the reasons XGBoost achieves performance and efficiency gains over other implementations of gradient boosting.\n",
        "\n",
        "In the previous exercise, the inputted datasets were converted into DMatrix data when we fit the data with `.fit()`. However, to use the `xgboost.cv()` object, we need to first explicitly convert the data into a `DMatrix`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWdInM2iegsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert X and y into a DMatrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBe18zQ3egsq",
        "colab_type": "text"
      },
      "source": [
        "### **Cross validation with xgb.cv**\n",
        "\n",
        "Note: Scikit-learn uses `n_estimator` to refer to the number of boosting rounds, or the number of trees in the model. In XGBoost, it's referred to as `num_boost_rounds`.\n",
        "\n",
        "To define the booster parameters, it's common practice to create a dictionary to hold all the parameters related to the individual boosters. We'll do this and define the objective functions as `binary:logistic` and the maximum tree depth to be `3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l09lFtiEegsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define booster parameters using a dictionary\n",
        "\n",
        "\n",
        "# Instantiate a CV object with 3 folds and 10 boosting rounds\n",
        "\n",
        "\n",
        "# Inspect the results: how are they stored?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7IXwTpDegss",
        "colab_type": "text"
      },
      "source": [
        "#### **Results in XGBoost**\n",
        "\n",
        "There are ten rows for our ten boosting rows.\n",
        "\n",
        "`error` as defined by [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters):\n",
        "> Binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n",
        "\n",
        "This means we need to subtract the last boosting round's `test-error-mean` from 1 to get the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tWuoJd4egst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Caclulate accuracy\n",
        "\n",
        "\n",
        "# Print the baseline accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz7QsHcmegsv",
        "colab_type": "text"
      },
      "source": [
        "Let's now look into improving performace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bWZUtgegsv",
        "colab_type": "text"
      },
      "source": [
        "### **More trees with early stopping**\n",
        "\n",
        "We've been using ten trees (aka `num_boost_rounds`). Let's add more trees (40 instead of 10), but make sure to add **early stopping**. \n",
        "\n",
        "Early stopping works by testing the model after every boosting round against the holdout set. If the holdout metric (error in our case) has not improved after a given number of rounds (defined by `early_stopping_rounds`), then any additional boosting rounds are stopped. If the model continuously improves up to `num_boost_round`, then early stopping does not occur.\n",
        "\n",
        "This helps automatically select the number of boosting rounds and minimize additional training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zogeN2hlegsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Increase the number of trees to 40 and set the early stopping rounds to 10\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "\n",
        "\n",
        "# Print the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58XpiQ2jegs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Did early stopping happen?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC6V1hQ8egs5",
        "colab_type": "text"
      },
      "source": [
        "There are 40 rows, and the test error seems to have decreased almost continuously after each round. This means early stopping did not happen because our model kept improving.\n",
        "\n",
        "We've improved the results by increasing the number of boosted trees, but there are more parameters we can play with!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D85B-1HXegs5",
        "colab_type": "text"
      },
      "source": [
        "## **4. Digging into Parameters**\n",
        "\n",
        "Along with setting the number of boosting rounds and early stopping rounds, there are many other parameters for our tree booster. We can see this in the documentation for [parameters for tree booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster).\n",
        "\n",
        "In this section, we'll take a look at several important parameters and understand what they do.\n",
        "\n",
        "For this session's purpose, we'll use `XGBClassifier()` with **25 boosting** rounds to avoid long training times. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QbXgc2oegs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Instantiate the XGBClassifier with 25 boosting rounds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxd2UvBaegs-",
        "colab_type": "text"
      },
      "source": [
        "### **Max depth**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "> Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/max_depth.png?raw=true\" width = \"35%\"> \n",
        "</p>\n",
        "\n",
        "Let's see what happens when we increase the `max_depth` from 3 to 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF-z7vY_egs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set max_depth to 10\n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ug_cyUegtB",
        "colab_type": "text"
      },
      "source": [
        "### **colsample_bytree**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "\n",
        "> The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
        "\n",
        "Essentially, this lets us limit the number of columns used when constructing each tree. This adds randomness, making the model more robust to noise. The default is 1 (i.e. all the columns), let's try a smaller value.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/colsample_bytree.gif?raw=true\" width = \"55%\"> \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR3ZeImPegtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set colsample_bytree to 0.5 \n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffKcMocegtE",
        "colab_type": "text"
      },
      "source": [
        "We can also limit the number of columns used by every depth level or node of our tree.\n",
        "\n",
        "_From XGBoost docs:_\n",
        "\n",
        "> `colsample_bylevel` is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n",
        "\n",
        "> `colsample_bynode` is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc2NtL3WegtE",
        "colab_type": "text"
      },
      "source": [
        "### **subsample**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "\n",
        "> - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. \n",
        "> - Subsampling will occur once in every boosting iteration.\n",
        "> - range: (0,1]\n",
        "\n",
        "Default is 1, let's try 0.75. \n",
        "\n",
        "This means each of our 25 trees will get a random sampling of 75% of our training data. Each tree will train on different portions of data which adds randomness (similar to `colsample_bytree`). \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/subsample.gif?raw=true\" width = \"55%\"> \n",
        "</p>\n",
        "\n",
        "However, we don't want this too low if we don't have many trees, because our model will underfit from not seeing as much data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_TNOis3egtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set subsample to 0.75 \n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TQEfG-hegtK",
        "colab_type": "text"
      },
      "source": [
        "### **gamma**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "> - Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
        "> - range: [0,∞]\n",
        "\n",
        "\n",
        "This decides whether a node will split based on the expected loss reduction loss after the split. `gamma` represents the minimum loss reduction required for a node to split.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/gamma.png?raw=true\" width = \"55%\"> \n",
        "</p>\n",
        "\n",
        "Increasing `gamma` = less splits = less complexity\n",
        "\n",
        "The default is 0, so in our case, nodes have always split until the maximum depth. Let's increase it to 0.25.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXuKmQwKegtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set gamma to .25 \n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp92TsxLegtQ",
        "colab_type": "text"
      },
      "source": [
        "### **Learning Rate (aka eta)**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "> - Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
        "> - range: [0,1]\n",
        "\n",
        "The learning rate affects how quickly a model learns.\n",
        "\n",
        "Gradient boosting works by sequentially adding weak learners to the model. Each new weak learner attempts to correct the residual errors from the preceding trees. This make the model very susceptible to overfitting. Learning rate can help slow down learning by shrinking the resulting weights of the current tree before passing them on to the next tree.\n",
        "\n",
        "The learning rate of our model is currently 0.1. What happens if we change it to 0.3?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ86u1PRegtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set learning rate to .3 \n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIQ9UJzjDERA",
        "colab_type": "text"
      },
      "source": [
        "The learning rate and the number of trees should be tuned together. If we decrease the learning rate, we need to make sure we have enough trees to learn something and avoid severely underfitting. **Therefore, a lower learning rate will require more boosting rounds.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afmWgrWCegtM",
        "colab_type": "text"
      },
      "source": [
        "### **reg_alpha**\n",
        "\n",
        "_From XGBoost docs:_\n",
        "> L1 regularization term on weights. Increasing this value will make model more conservative.\n",
        "\n",
        "L1 is often referred to as **lasso regression**. It's a foundational regularization technique, meaning it aims to reduce overfitting by discouraging complex models. In the case of gradient boosting, L1 does this by adding penalties on leaf weights. Increasing `alpha` drives base learners' leaf weights towards 0.\n",
        "\n",
        "Default is 0, meaning there is no alpha regularization in our model currently. Let's activate L1 with a value of `0.01`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1skhV5NegtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set reg_alpha to .1 \n",
        "\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "\n",
        "\n",
        "# Predict the labels of the test set\n",
        "\n",
        "\n",
        "# Compute the accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDd4PjwCegtP",
        "colab_type": "text"
      },
      "source": [
        "**L2**, aka ridge regression, is also available with the parameter `reg_lambda`. L2 is known for having smoother penalty then L1. This means leaf weights smoothly decrease rather with less risk of sparsity in the leaf weights. So, make sure to try our different regularization techniques! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJUwZYohz4Wu",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our resulting parameters after we manually changed them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01h-970eegtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the model parameters\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc-k9UF1egtY",
        "colab_type": "text"
      },
      "source": [
        "There are a lot of possible parameters combinations. We can't manually tune and pick them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZxdStQKtJC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 3</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-BrJrQJegtY",
        "colab_type": "text"
      },
      "source": [
        "## **5. Hyperparameter tuning with Random Search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8w5QLKtegtZ",
        "colab_type": "text"
      },
      "source": [
        "Grid search and random search are the most popular methods for hyperparameter tuning. However, grid search can get computationally expensive if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. In this last section, this is why we'll use random search because it doesn't try all the hyperparameter values. In random search, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n",
        "\n",
        "XGBoost doesn't have a built-in gridsearch function, so we need to use `scikit-learn`'s [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). This means we'll have to use `XGBClassifier()` because it's `scikit-learn` compatible.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/randomized_search_1.gif?raw=true\" width = \"50%\"> \n",
        "</p>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "The `RandomizedSearchCV()` function takes in the following arguments:\n",
        "\n",
        "- `estimator`: The estimator being fit, here it's XGBoost.\n",
        "- `param_distributions`: Unlike `params` - this is the distribution of possible hyperparameters to use.\n",
        "- `cv`: Number of cross-validation iterations\n",
        "- `n_iter`: Number of hyperparameter combinations to choose from\n",
        "- `verbose`: Prints more output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbhMqdeBegtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Define a parameter grid\n",
        "rs_param_grid = {\n",
        "    # max_depth: values from 3 to 12\n",
        "\n",
        "    # alpha: values 0, .001, .01, .1\n",
        "\n",
        "    # subsample: values 0.25,0.5,0.75, 1\n",
        "\n",
        "    # learning rate: ten values between 0.01 - 0.5\n",
        "\n",
        "    # n_estimators: values 10, 25, 40\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "# Insantiate XGBoost Clasifier \n",
        "\n",
        "\n",
        "# Instantiate RandomizedSearchCV()\n",
        "\n",
        "\n",
        "# Train the model on the training set\n",
        "\n",
        "\n",
        "# Print the best parameters and highest accuracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8E_P0_nzhNd",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 4</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dRDfVHuegtd",
        "colab_type": "text"
      },
      "source": [
        "## **Take home assignment**\n",
        "\n",
        "With more time at home, what is the highest accuracy you can reach on the test set (`X_test`,`y_test`) after training on the training set (`X_train`,`y_train`)? Make sure to play around with the parameters and their values in `rs_param_grid`.\n",
        "\n",
        "Send me your results and code either by email (lis@datacamp.com) or through [LinkedIn](https://www.linkedin.com/in/elisabethsulmont/). Good luck!"
      ]
    }
  ]
}