{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ijg5wUCTQYG"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
    "</p>\n",
    "<br><br>\n",
    "\n",
    "# **Machine Learning with XGboost**\n",
    "\n",
    "Welcome to this hands-on training where we will learn how to use XGBoost to create powerful prediction models using gradient boosting. Using Jupyter Notebooks you'll learn how to efficiently create, evaluate, and tune XGBoost models. This session will run for three hours, allowing you time to really immerse yourself in the subject, and includes short breaks and opportunities to ask the expert questions throughout the training. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- How to instantiate and customize XGBoost models\n",
    "- How to use XGBoost's DMatrix to optimize performance\n",
    "- How to evaluate models in XGBoost using the right metrics\n",
    "- How to tune parameters in XGBoost to achieve the best results\n",
    "- How to visualize trees in XGBoost to analyze feature importance\n",
    "\n",
    "\n",
    "## **The Dataset**\n",
    "\n",
    "The dataset to be used in this webinar is a CSV file named `hotel_bookings_clean.csv`, which contains data on hotel bookings. \n",
    "\n",
    "### **Acknowledgements**\n",
    "The dataset was downloaded on [Kaggle](https://www.kaggle.com/jessemostipak/hotel-booking-demand/). The data is originally from an article called [Hotel booking demand datasets](https://www.sciencedirect.com/science/article/pii/S2352340918315191) by Nuno Antonio, Ana de Almeida, and Luis Nunes. It was then cleaned by Thomas Mock and Antoine Bichat for [#TidyTuesday during the week of February 11th, 2020](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md). For the purposes of this live training, it was further pre-processed to have cleaner ready-to-use features (e.g., dropping of irrelevant columns, one-hot-encoding). The dataset has the following [license](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "### **Data Dictionary**\n",
    "\n",
    "It contains the **53 columns**:\n",
    "\n",
    "_For binary variables: `1` = true and `0` = false._\n",
    "\n",
    "#### **Target** \n",
    "- `is_cancelled`: Binary variable indicating whether a booking was canceled\n",
    "\n",
    "#### **Features** \n",
    "- `lead time`: Number of days between booking date and arrival date\n",
    "- `arrival_date_week_number`, `arrival_date_day_of_month`, `arrival_date_month`: Week number, day date, and month number of arrival date \n",
    "- `stays_in_weekend_nights`, `stays_in_week_nights`: Number of weekend nights (Saturday and Sunday) and week nights (Monday to Friday) the customer booked\n",
    "- `adults`,`children`,`babies`: Number of adults, children, babies booked for the stay\n",
    "- `is_repeated_guest`: Binary variable indicating whether the customer was a repeat guest \n",
    "- `previous_cancellations`: Number of prior bookings that were canceled by the customer\n",
    "- `previous_bookings_not_canceled`: Number of prior bookings that were not canceled by the customer\n",
    "- `required_car_parking_spaces`: Number of parking spaces requested by the customer\n",
    "- `total_of_special_requests`: Number of special requests made by the customer\n",
    "- `avg_daily_rate`: Average daily rate, as defined by dividing the sum of all lodging transactions by the total number of staying nights\n",
    "- `booked_by_company`: Binary variable indicating whether the booking was booked by a company               \n",
    "- `booked_by_agent`: Binary variable indicating whether the booking was booked by an agent \n",
    "- `hotel_city Hotel`: Binary variable indicating whether the booked hotel is a \"City Hotel\"\n",
    "- `hotel_Resort Hotel`: Binary variable indicating whether the booked hotel is a \"Resort Hotel\"\n",
    "- `meal_BB`: Binary variable indicating whether a bed & breakfast meal was booked \n",
    "- `meal_HB`: Binary variable indicating whether a half board meal was booked\n",
    "- `meal_FB`: Binary variable indicating whether a full board meal was booked \n",
    "- `meal_No meal`:Binary variable indicating whether there was no meal package booked \n",
    "- `market_segment_Aviation`, `market_segment_Complementary`, `market_segment_Corporate`, `market_segment_Direct`, `market_segment_Groups`, `market_segment_Offline TA/TO`, `market_segment_Online TA`, `market_segment_Undefined`: Indicates market segment designation with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators\n",
    "- `distribution_channel_Corporate`, `distribution_channel_Direct`, `distribution_channel_GDS`, `distribution_channel_TA/TO`, `distribution_channel_Undefined`: Indicates booking distribution channel with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators, \"GDS\" = Global Distribution System\n",
    "- `reserved_room_type_A`,`reserved_room_type_B`, `reserved_room_type_C`,`reserved_room_type_D`, `reserved_room_type_E`, `reserved_room_type_F`, `reserved_room_type_G`, `reserved_room_type_H`, `reserved_room_type_L`: Indicates code of room type reserved with a value of `1`. Code is presented instead of designation for anonymity reasons\n",
    "- `deposit_type_No Deposit`: Binary variable indicating whether a deposit was made\n",
    "- `deposit_type_Non Refund`: Binary variable indicating whether a deposit was made in the value of the total stay cost\n",
    "- `deposit_type_Refundable`: Binary variable indicating whether a deposit was made with a value under the total stay cost \n",
    "- `customer_type_Contract`: Binary variable indicating whether the booking has an allotment or other type of contract associated to it \n",
    "- `customer_type_Group`: Binary variable indicating whether the booking is associated to a group \n",
    "- `customer_type_Transient`: Binary variable indicating whether the booking is not part of a group or contract, and is not associated to other transient booking\n",
    "- `customer_type_Transient-Party`: Binary variable indicating whether the booking is transient, but is associated to at least other transient booking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMYfcKeDY85K"
   },
   "source": [
    "## **1. Getting to know our data**\n",
    "\n",
    "Let's get to know our columns and split our data into features and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMQfyC7GUNhT"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb # XGBoost typically uses the alias \"xgb\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "l8t_EwRNZPLB",
    "outputId": "36a85c6f-f2ae-44e0-ac01-fc55462bc616"
   },
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "bookings = pd.read_csv('https://raw.githubusercontent.com/datacamp/Machine-Learning-With-XGboost-live-training/master/data/hotel_bookings_clean.csv')\n",
    "\n",
    "# List out our columns\n",
    "bookings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have 52 columns with 119,210 rows. All the datatypes are numeric and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAfz_jiu0NjN"
   },
   "outputs": [],
   "source": [
    "# Take a closer look at column distributions\n",
    "bookings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot cancellation counts to visualize proportion of not cancelled and cancelled\n",
    "bookings['is_canceled'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember for our binary variables, like `is_canceled`, `1` = true and `0` = false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an exact percentage of not cancelled and cancelled\n",
    "(bookings['is_canceled'].value_counts()/bookings['is_canceled'].count())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Which features are most correlated to cancelations?**\n",
    "\n",
    "#### **Correlation Coefficient**\n",
    "- Quantifies the linear relationship between two variables\n",
    "- Number between -1 and 1\n",
    "- Magnitude corresponds to strength of relationship\n",
    "- Sign (+ or -) corresponds to direction of relationship\n",
    "- Most common way to calculate: **Pearson product-moment correlation coefficient**\n",
    "    - ${\\displaystyle \\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}}$\n",
    "\n",
    "![Plots displaying different levels of correlation](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/correlation.png?raw=true)\n",
    "\n",
    "We can use `pandas`'s [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) `DataFrame.corr()` which returns a correlation matrix using the Pearson correlation coefficient as default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix \n",
    "bookings.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are most correlated with `is_cancelled`?\n",
    "bookings.corr()['is_canceled'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be cautious, correlation does not equal feature importance! Correlation may not neccesarily help differentiate classes. Also, the Pearson coefficient only considers **linear** relationships and some of these variables are binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting data**\n",
    "Let's split our label and features so we can get to building models! The first column is our label `is_cancelled`, the rest are features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X, y = bookings.iloc[:,1:], bookings.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Your First XGBoost Classifier**\n",
    "\n",
    "XGBoost has a  [scikit-learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn), which is useful if you want to use different scikit-learn classes and methods on an XGBoost model (e.g.,`predict()`, `fit()`).  In this section, we'll try the API out with the `xgboost.XGBClassifier()` class and get a baseline accuracy for the rest of our work. \n",
    "\n",
    "As a reminder, gradient boosting sequentially trains weak learners where each weak learner tries to correct its precedessor's mistakes.  We want our weak learners, or boosters, to be decision trees, so we'll explicitly define our booster to be `gbtree`. So that our results are reproducible, we'll set the `random_state=123`.\n",
    "\n",
    "First we'll instantiate a simple XGBoost classifier without changing any of the other paremeters and we'll inspect the parameters that we haven't touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train and test split using sklearn\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.33, random_state=123)\n",
    "\n",
    "# Instatiate a XGBClassifier with gbtree as the booster \n",
    "xgb_clf = xgb.XGBClassifier(booster=\"gbtree\", random_state=123)\n",
    "\n",
    "# Inspect the parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple things to note:\n",
    "- The `objective` function, or loss function, is defined as `binary:logistic`. The objective fuction quantifies how far off a prediction is from the actual results. We want to minimize this to have the smallest possible loss. `binary:logistic` is the default for classifiers. `binary:logistic` outputs the actual predicted probability of the positive class (in our case, that a booking is cancelled).\n",
    "- `n_estimators` is the number of gradient boosted trees we want in our model. It's equivalent to the number of boosting rounds. We don't want too many boosting rounds because our model will overfit, e.g., memorize the training data. **Let's lower it from 100 to 10**.\n",
    "\n",
    "<img style=\"float: center; width:75%\" src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/n_estimators.png?raw=true\"> \n",
    "\n",
    "- `max_depth` is the maximum tree depth allowed. Tree depth is the length of the longest path from the root node to a leaf node. Making this too high will give our model more variance, or more potential to overfit. Similar to `n_estimators` the more we increase this, the longer our training period will be. **Let's set this to 3**.\n",
    "\n",
    "<img style=\"float: center; width:25%\" src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/max_depth.png?raw=true\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set n_estimator and max_depth\n",
    "xgb_clf.set_params(max_depth=3, n_estimators=10)\n",
    "\n",
    "# Fit it to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we are going to use **accuracy** as our metric, since we only have two classes and the smallest class takes ~37% of the data. To calculate accuracy we need to count the correctly predicted data points and divide by the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "\n",
    "# Print the baseline accuracy\n",
    "print(\"Baseline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing your tree**\n",
    "\n",
    "`XGBoost` has two handy visualization functions for interpreting results.\n",
    "\n",
    "The first is `plot_importance()`  which plots feature importance, meaning, how predictive each feature is for the target variable. It takes in the fitted XGBoost model fitted and the subplot axis in the `ax` argument.\n",
    "\n",
    "\n",
    "#### **Plotting feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8)\n",
    "\n",
    "xgb.plot_importance(xgb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is importance calculated? Here's an excerpt from the [documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting):\n",
    "\n",
    "> **importance_type (str, default \"weight\") –**\n",
    "> How the importance is calculated: either “weight”, “gain”, or “cover”\n",
    "> - ”weight” is the number of times a feature appears in a tree\n",
    "> - ”gain” is the average gain of splits which use the feature\n",
    "> - ”cover” is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split\n",
    "\n",
    "We'll focus on the first two. Gain is way to quantify how much a feature contributes to improving accuracy. Let's try out `gain` to see how it differs from `weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gain instead of weight\n",
    "xgb.plot_importance(xgb_clf, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the ordering of features differs quite a bit between `gain` and `weight`! This implies that (1) feature importance can be subjective and (2) the number of appearances a feature has on a tree is not necessarily correlated to how much gain it brings. For example, a binary variable has less of a chance to appear as many times as a continuous variable on a tree, since there are only two outputs. However, it can still can be a powerful feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting a decision tree from your model**\n",
    "\n",
    "The second handy visualization function is`plot_tree()` which visually generates a decision tree in your model. It takes in the fitted XGBoost model and with the `num_trees` parameter you can indicate which tree you want to see. For example, if I have `n_estimator=3`, I can set the parameter as `0`, `1`, or `2` to see either the first, second, or third tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first tree\n",
    "xgb.plot_tree(xgb_clf, num_trees=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a higher resolution version of the tree [here](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/tree1.png?raw=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a parameter called `rankdir`. We can set it to `LR` so the decision builds left to right, as opposed to top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the last tree\n",
    "xgb.plot_tree(xgb_clf, num_trees=9, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a higher resolution version of the tree [here](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/tree2.png?raw=true)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Cross Validation in XGBoost**\n",
    "\n",
    "Cross validation is considered best practice for assessing a model's performance. We can use `xgboost.cv()` to efficiently run cross validation on XGBoost models. This method is part of XGBoost's core library and **not** part of XGBoost's scikit-learn API from earlier. \n",
    "\n",
    "<img style=\"float: center; width:75%\" src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/cv.png?raw=true\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting to DMartix**\n",
    "\n",
    "DMatrix is XGBoost's internal data structure designed to optimize for both memory efficiency and training speed. It's one of the reason XGBoost achieves performance and efficiency gains over other implementations of gradient boostng.\n",
    "\n",
    "In the previous exercise, the inputed datasets were converted into DMatrix data when we fit the data with `.fit()`. However, to use the `xgboost.cv()` object, we need to first explicitly convert the data into a `DMatrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y into a DMatrix\n",
    "bookings_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross validation with xgb.cv**\n",
    "\n",
    "Note: Scikit-learn uses `n_estimator` to refer to the number of boosting rounds or number of gradient boosted trees. In XGBoost, it's referred to as `num_boost_rounds`.\n",
    "\n",
    "To define the booster parameters, it's common practice to create a dictionary to hold all the parameters related to the individual boosters. We'll do this and define the objective functions to be `binary:logistic` and the maximum tree depth to be `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define booster parameters using a dictionary\n",
    "params = {\"objective\":\"binary:logistic\", 'max_depth': 3}\n",
    "\n",
    "# Instantiate a CV object with 3 folds and 10 boosting rounds\n",
    "xgb_cv = xgb.cv(dtrain=bookings_dmatrix, params=params, nfold=3, num_boost_round=10, seed=123)\n",
    "\n",
    "# Inspect the results: how are they stored?\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results in XGBoost**\n",
    "\n",
    "There are 10 rows for our 10 boosting rows.\n",
    "\n",
    "`error` as defined by [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters):\n",
    "> Binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n",
    "\n",
    "This means we need to subtract the last boosting round's `test-error-mean` from 1 to get the accurac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result from last boosting round\n",
    "results = xgb_cv.tail(1)\n",
    "\n",
    "# Caclulate accuracy\n",
    "accuracy= 1 - xgb_cv[\"test-error-mean\"].iloc[-1]\n",
    "\n",
    "# Print the baseline accuracy\n",
    "print(\"baseline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to our `xgboost.XGBClassifier()` baseline which used the same booster parameters. This helps validate its performance. Let's now look into improving performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **More trees with early stopping**\n",
    "\n",
    "We've been using 10 trees (aka `num_boost_rounds`). Let's add more trees (40 instead of 10), but make sure to add **early stopping**. \n",
    "\n",
    "Early stopping works by testing the model after every boosting round against the holdout set and if the holdout metric (error in our case) has not improved after a given number of rounds (defined by `early_stopping_rounds`), then any additional boosting rounds are stopped. If the model continuously improves up to `num_boost_round`, then early stopping does not occur.\n",
    "\n",
    "This helps automatically select the number of boosting rounds and minimize unnecessary training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of trees to 40 and set the early stopping rounds to 10\n",
    "xgb_cv = xgb.cv(dtrain=bookings_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=40,early_stopping_rounds=10, seed=123)\n",
    "\n",
    "# Caclulate accuracy\n",
    "accuracy= 1 - xgb_cv[\"test-error-mean\"].iloc[-1]\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did early stopping happen?\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 40 rows and the test error seems to have decreased almost continuously after each round. This means early stopping did not happen because our model kept improving.\n",
    "\n",
    "We've improved the results by increasing the number of boosted trees, but there are more parameters we can play with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Digging into Parameters**\n",
    "\n",
    "Along with setting the number of boosting rounds and early stopping rounds, there are many other parameters for our tree booster. We can see this in the documentation for [parameters for tree booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster).\n",
    "\n",
    "In this section, we'll take a look at several important parameters and get an understanding of what they do.\n",
    "\n",
    "For the purpose of this session, we'll use `XGBClassifier()` with 25 boosting rounds to avoid long training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the XGBClassifier with 25 boosting rounds\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Max depth**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
    "\n",
    "Let's see what happens when we increase the max_depth from 3 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_depth to 6\n",
    "xgb_clf.set_params(max_depth=6)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **colsample_bytree**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "\n",
    "Essentially, this lets us limit the number of columns used when constructing each tree. This adds randomness, making the model more robust to noise. The default is 1 (i.e. all the columns), let's try a smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colsample_bytree to 0.5 and print accuracy\n",
    "xgb_clf.set_params(colsample_bytree=0.5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also limit the number of columns used by every depth level or node of our tree.\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> `colsample_bylevel` is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n",
    "\n",
    "> `colsample_bynode` is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **subsample**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. \n",
    "> - Subsampling will occur once in every boosting iteration.\n",
    "> - range: (0,1]\n",
    "\n",
    "Default is 1, let's try 0.75. \n",
    "\n",
    "This means each of our 10 trees will get a random sampling of 75% of our training data. This means that each tree will train on different portions of data and adds randomness (similar to `colsample_bytree`). However, we don't want this too low if we don't have many trees, because our model will underfit from not seeing as much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subsample to 0.75 and print accuracy\n",
    "xgb_clf.set_params(subsample=0.75)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **min_child_weight**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> - Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. \n",
    "> - The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "> - range: [0,∞]\n",
    "\n",
    "\n",
    "Let's increase `min_child_weight`. This will decrease model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set min_child_weight to 1.5 and print accuracy\n",
    "xgb_clf.set_params(min_child_weight=1.5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **gamma**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> - Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "> - range: [0,∞]\n",
    "\n",
    "Default is 0. Let's increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gamma to .25 and print accuracy\n",
    "xgb_clf.set_params(gamma=0.25)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **alpha**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "Default is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha to .1 and print accuracy\n",
    "xgb_clf.set_params(reg_alpha=0.01)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 is also available with `reg_lambda`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning Rate (aka eta)**\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> - Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "> - range: [0,1]\n",
    "\n",
    "Default is 0.30. What happens if we decrease it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate to .1 and print accuracy\n",
    "xgb_clf.set_params(learning_rate=0.1)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of possible parameters combinations. We can't manually tune and pick them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Hyperparameter tuning with Random Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search and random search are the most popular methods for hyperparameter tuning. However, grid search can get computationally expensive if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. In this last section, this is why we'll use random search because not all hyperparemeter values are tried. In random search, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n",
    "\n",
    "XGBoost doesn't have a built-in gridsearch function, so we need to use `scikit-learn`'s [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). This means we'll have to use `XGBClassifier()` because it's `scikit-learn` compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Create a parameter grid for three parameters\n",
    "## max_depth: values from 2 to 12\n",
    "## alpha: values 0, .001, .01, .1\n",
    "## subsample: values 0.25,0.5,0.75, 1\n",
    "## min_child_weight: values 1, 1.5, 2\n",
    "rs_param_grid = {\n",
    "    'max_depth': list((range(2,12))),\n",
    "    'alpha': [0,0.001, 0.01,0.1,1],\n",
    "    'subsample': [0.5,0.75,1],\n",
    "    'min_child_weight': [1,1.5,2]\n",
    "}\n",
    "\n",
    "# instantiate a new XGBClassifier()\n",
    "## Let's keep at n_estimator to 20 to keep computation time low(er)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=20, random_state=123)\n",
    "\n",
    "# Instantiate RandomizedSearchCV()\n",
    "## For random search, we use param_distributions instead of params\n",
    "## n_iter: the number of parameter settings that are tried\n",
    "## verbose: get more output on what's being computers\n",
    "xgb_rs = RandomizedSearchCV(estimator=xgb_clf,param_distributions=rs_param_grid, \n",
    "                                cv=3, n_iter=5, verbose=2, random_state=123)\n",
    "\n",
    "# Train the model on the training set\n",
    "xgb_rs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", xgb_rs.best_params_)\n",
    "print(\"Best accuracy found: \", xgb_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Take home assignment**\n",
    "\n",
    "The above code takes about ~3 minutes to run and reaches an accuracy of ~0.8403 on the training data (`X_train`,`y_train`).\n",
    "\n",
    "With less time restrictions at home, what is the highest accuracy you can reach on the test set (`X_test`,`y_test`)? Make sure to play around with the parameters and their values in `rs_param_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "python_live_session_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
